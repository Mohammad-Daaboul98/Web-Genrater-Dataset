{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SHqig9seHA9",
        "outputId": "75d7084b-5196-44ea-b404-300f9df0fe3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kKj9svjzP995"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SCGsM0CmxhKn"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "'\\n': 0,\n",
        "'<START>': 1,\n",
        "'<END>': 2,\n",
        "'}': 3,\n",
        "'{': 4,\n",
        "',': 5,\n",
        "' ': 6,\n",
        "'header': 7,\n",
        "'navbar-dark': 8,\n",
        "'navbar-light': 9,\n",
        "'navbar-primary': 10,\n",
        "'navbar-warning': 11,\n",
        "'navbar-danger': 12,\n",
        "'navbar-secondary': 13,\n",
        "'navbar-success': 14,\n",
        "'navbar-brand': 15,\n",
        "'navbar-toggler': 16,\n",
        "'navbar-toggler-icon': 17,\n",
        "'navbar-collapse': 18,\n",
        "'navbar-nav': 19,\n",
        "'nav-item': 20,\n",
        "'nav-item-active': 21,\n",
        "'nav-item-unactive': 22,\n",
        "'nav-link': 23,\n",
        "'nav-link-undisabled': 24,\n",
        "'nav-link-disabled': 25,\n",
        "'nav-link-active': 26,\n",
        "'form-inline-success': 27,\n",
        "'form-inline-danger': 28,\n",
        "'form-inline-primary': 29,\n",
        "'form-primary': 30,\n",
        "'form-danger': 31,\n",
        "'form-inline-secondary': 32,\n",
        "'form-secondary': 33,\n",
        "'form-inline-warning': 34,\n",
        "'form-inline-dark': 35,\n",
        "'form-dark': 36,\n",
        "'form-inline-light': 37,\n",
        "'from-d-flex': 38,\n",
        "'form-control-me-2': 39,\n",
        "'container-fluid': 40,\n",
        "'container': 41,\n",
        "'container-marketing': 42,\n",
        "'main': 43,\n",
        "'main-container':44,\n",
        "'carousel-slide': 45,\n",
        "'row': 46,\n",
        "'col-lg-4': 47,\n",
        "'col-md-7': 48,\n",
        "'col-md-5': 49,\n",
        "'order-md-2': 50,\n",
        "'svg-140px': 51,\n",
        "'svg-500px': 52,\n",
        "'svg-250px': 53,\n",
        "'svg': 54,\n",
        "'h-1': 55,\n",
        "'h-2': 56,\n",
        "'h-3': 57,\n",
        "'h-4': 58,\n",
        "'h-5': 59,\n",
        "'h-6': 60,\n",
        "'h-span': 61,\n",
        "'h-featurette': 62,\n",
        "'paragraph': 63,\n",
        "'paragraph-lead': 64,\n",
        "'p-btn': 65,\n",
        "'btn-primary': 66,\n",
        "'btn-success': 67,\n",
        "'btn-secondary': 68,\n",
        "'btn-dark': 69,\n",
        "'btn-warning': 70,\n",
        "'btn-danger': 71,\n",
        "'btn-outline-success': 72,\n",
        "'btn-outline-danger': 73,\n",
        "'btn-outline-dark': 74,\n",
        "'featurette-divider': 75,\n",
        "'br': 76,\n",
        "'h5-my-0': 77,\n",
        "'nav-my-2': 78,\n",
        "'a-text-dark': 79,\n",
        "'d-flex': 80,\n",
        "'text-center': 81,\n",
        "'card-deck': 82,\n",
        "'card': 83,\n",
        "'card-mb-4-shadow-sm': 84,\n",
        "'card-header': 85,\n",
        "'card-body': 86,\n",
        "'card-title': 87,\n",
        "'list-unstyled': 88,\n",
        "'li': 89,\n",
        "'btn-outline-primary': 90,\n",
        "'album': 91,\n",
        "'album-py-5-bg-light': 92,\n",
        "'col-md-4': 93,    \n",
        "'svg-bd-placeholder-img-card-img-top': 94,\n",
        "'p-card-text': 95,\n",
        "'d-flex-center': 96,\n",
        "'btn-group': 97,\n",
        "'btn-outline-secondary': 98,\n",
        "'small-text-muted': 99,\n",
        "'nav-item-dropdown': 100,\n",
        "'nav-link-dropdown-toggle': 101,\n",
        "'dropdown-menu': 102,\n",
        "'dropdown-item': 103,\n",
        "'form-inline': 104,\n",
        "'jumbotron': 105,\n",
        "'hr': 106,\n",
        "'text-muted': 107,\n",
        "'section-jumbotron-text-center': 108,\n",
        "'justify-content-center': 109,\n",
        "'align-items-center': 110,\n",
        "'collapse-bg-dark': 111,\n",
        "'col-sm-8-col-md-7-py-4': 112,\n",
        "'col-sm-4-offset-md-1-py-4': 113,\n",
        "'h-4-text-white': 114,\n",
        "'paragraph-lead-text-muted': 115,\n",
        "'list-unstyled-only': 116,\n",
        "'a-text-white': 117,\n",
        "'navbar-dark-bg-dark-shadow': 118,\n",
        "'d-flex-justify-content-between': 119,\n",
        "'navbar-toggler-icon-span': 120,\n",
        "'a-navbar-brand-d-flex-align-items-center': 121,\n",
        "'jumbotron-center': 122,\n",
        "'h-jumbotron': 123,\n",
        "'card-shadow': 124,\n",
        "'paragraph-card-text': 125,\n",
        "'card-img-top': 126,\n",
        "'card-mb-4-shadow': 127,\n",
        "'btn-outline-secondary-sm': 128,\n",
        "'btn-primary-my-2': 129,\n",
        "'btn-secondary-my-2': 130,\n",
        "'navbar-collapse-dark': 131,\n",
        "'navbar-collapse-primary': 132,\n",
        "'navbar-collapse-success': 133,\n",
        "'navbar-collapse-danger': 134,\n",
        "'navbar-collapse-warning': 135,\n",
        "'navbar-collapse-secondary': 136,\n",
        "'navbar-collapse-light': 137,\n",
        "'collapse-bg-primary': 138,\n",
        "'collapse-bg-secondary': 139,\n",
        "'collapse-bg-success': 140,\n",
        "'collapse-bg-danger': 141,\n",
        "'collapse-bg-info': 142,\n",
        "'paragraph-lead-light': 143,\n",
        "'navbar-dark-bg-primary-shadow': 144,\n",
        "'navbar-dark-bg-secondary-shadow': 145,\n",
        "'navbar-dark-bg-success-shadow': 146,\n",
        "'navbar-dark-bg-danger-shadow': 147,\n",
        "'navbar-dark-bg-info-shadow': 148,\n",
        "'':149\n",
        "}\n",
        "\n",
        "pickle.dump(data, open('voc.pkl' , 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qZ3ZeUstpjP",
        "outputId": "6cf97d01-92ad-4585-8c08-54aa94767e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'\\n': 0, '<START>': 1, '<END>': 2, '}': 3, '{': 4, ',': 5, ' ': 6, 'header': 7, 'navbar-dark': 8, 'navbar-light': 9, 'navbar-primary': 10, 'navbar-warning': 11, 'navbar-danger': 12, 'navbar-secondary': 13, 'navbar-success': 14, 'navbar-brand': 15, 'navbar-toggler': 16, 'navbar-toggler-icon': 17, 'navbar-collapse': 18, 'navbar-nav': 19, 'nav-item': 20, 'nav-item-active': 21, 'nav-item-unactive': 22, 'nav-link': 23, 'nav-link-undisabled': 24, 'nav-link-disabled': 25, 'nav-link-active': 26, 'form-inline-success': 27, 'form-inline-danger': 28, 'form-inline-primary': 29, 'form-primary': 30, 'form-danger': 31, 'form-inline-secondary': 32, 'form-secondary': 33, 'form-inline-warning': 34, 'form-inline-dark': 35, 'form-dark': 36, 'form-inline-light': 37, 'from-d-flex': 38, 'form-control-me-2': 39, 'container-fluid': 40, 'container': 41, 'container-marketing': 42, 'main': 43, 'main-container': 44, 'carousel-slide': 45, 'row': 46, 'col-lg-4': 47, 'col-md-7': 48, 'col-md-5': 49, 'order-md-2': 50, 'svg-140px': 51, 'svg-500px': 52, 'svg-250px': 53, 'svg': 54, 'h-1': 55, 'h-2': 56, 'h-3': 57, 'h-4': 58, 'h-5': 59, 'h-6': 60, 'h-span': 61, 'h-featurette': 62, 'paragraph': 63, 'paragraph-lead': 64, 'p-btn': 65, 'btn-primary': 66, 'btn-success': 67, 'btn-secondary': 68, 'btn-dark': 69, 'btn-warning': 70, 'btn-danger': 71, 'btn-outline-success': 72, 'btn-outline-danger': 73, 'btn-outline-dark': 74, 'featurette-divider': 75, 'br': 76, 'h5-my-0': 77, 'nav-my-2': 78, 'a-text-dark': 79, 'd-flex': 80, 'text-center': 81, 'card-deck': 82, 'card': 83, 'card-mb-4-shadow-sm': 84, 'card-header': 85, 'card-body': 86, 'card-title': 87, 'list-unstyled': 88, 'li': 89, 'btn-outline-primary': 90, 'album': 91, 'album-py-5-bg-light': 92, 'col-md-4': 93, 'svg-bd-placeholder-img-card-img-top': 94, 'p-card-text': 95, 'd-flex-center': 96, 'btn-group': 97, 'btn-outline-secondary': 98, 'small-text-muted': 99, 'nav-item-dropdown': 100, 'nav-link-dropdown-toggle': 101, 'dropdown-menu': 102, 'dropdown-item': 103, 'form-inline': 104, 'jumbotron': 105, 'hr': 106, 'text-muted': 107, 'section-jumbotron-text-center': 108, 'justify-content-center': 109, 'align-items-center': 110, 'collapse-bg-dark': 111, 'col-sm-8-col-md-7-py-4': 112, 'col-sm-4-offset-md-1-py-4': 113, 'h-4-text-white': 114, 'paragraph-lead-text-muted': 115, 'list-unstyled-only': 116, 'a-text-white': 117, 'navbar-dark-bg-dark-shadow': 118, 'd-flex-justify-content-between': 119, 'navbar-toggler-icon-span': 120, 'a-navbar-brand-d-flex-align-items-center': 121, 'jumbotron-center': 122, 'h-jumbotron': 123, 'card-shadow': 124, 'paragraph-card-text': 125, 'card-img-top': 126, 'card-mb-4-shadow': 127, 'btn-outline-secondary-sm': 128, 'btn-primary-my-2': 129, 'btn-secondary-my-2': 130, 'navbar-collapse-dark': 131, 'navbar-collapse-primary': 132, 'navbar-collapse-success': 133, 'navbar-collapse-danger': 134, 'navbar-collapse-warning': 135, 'navbar-collapse-secondary': 136, 'navbar-collapse-light': 137, 'collapse-bg-primary': 138, 'collapse-bg-secondary': 139, 'collapse-bg-success': 140, 'collapse-bg-danger': 141, 'collapse-bg-info': 142, 'paragraph-lead-light': 143, 'navbar-dark-bg-primary-shadow': 144, 'navbar-dark-bg-secondary-shadow': 145, 'navbar-dark-bg-success-shadow': 146, 'navbar-dark-bg-danger-shadow': 147, 'navbar-dark-bg-info-shadow': 148, '': 149}\n"
          ]
        }
      ],
      "source": [
        "with open('/content/voc.pkl','rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0Um499PoLNyq"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3)\n",
        "        self.conv5 = nn.Conv2d(64, 128, 3)\n",
        "        self.conv6 = nn.Conv2d(128, 128, 3)\n",
        "        self.fc1 = nn.Linear(in_features=128*28*28, out_features=1024)\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x -> [-1, 3, 256, 256]\n",
        "        \n",
        "        x = F.relu(self.conv1(x))\n",
        "        # x -> [-1, 32, 254, 254]\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # x -> [-1, 32, 252, 252]\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # x -> [-1, 32, 126, 126]\n",
        "        \n",
        "        x = F.relu(self.conv3(x))\n",
        "        # x -> [-1, 64, 124, 124]\n",
        "        x = F.relu(self.conv4(x))\n",
        "        # x -> [-1, 64, 122, 122]\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # x -> [-1, 64, 61, 61]\n",
        "\n",
        "        x = F.relu(self.conv5(x))\n",
        "        # x -> [-1, 128, 59, 59]\n",
        "        x = F.relu(self.conv6(x))\n",
        "        # x -> [-1, 128, 57, 57]\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # x -> [-1, 128, 28, 28]\n",
        "\n",
        "        x = x.view(-1, 128*28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "class ContextEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ContextEncoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=150, hidden_size=128, num_layers=2, batch_first=True)\n",
        "    \n",
        "    def forward(self, x, h=None):\n",
        "        # x -> [-1, seq_size, 19], h -> [num_layer=2,-1, 128]\n",
        "\n",
        "        if not h:\n",
        "            h = (torch.zeros((2, x.size(0), 128)).cuda(),\n",
        "                 torch.zeros((2, x.size(0), 128)).cuda())\n",
        "\n",
        "        x, _ = self.lstm(x, h)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1024+128, hidden_size=512, num_layers=2, batch_first=True)\n",
        "        self.l1 = nn.Linear(512, 150)\n",
        "    \n",
        "    def forward(self, image_feature, context_feature, on_cuda = False, h = None):\n",
        "        # image_feature -> [-1, 1024], context_feature -> [-1, seq_size=48, 128], h -> [num_layer=2, -1, 512]\n",
        "        image_feature = image_feature.unsqueeze(1)\n",
        "        # image_feature -> [-1, 1, 1024]\n",
        "        image_feature = image_feature.repeat(1, context_feature.size(1), 1)\n",
        "        # image_feature -> [-1, seq_size, 1024]\n",
        "        x = torch.cat((image_feature, context_feature), 2)\n",
        "        # x -> [-1, seq_size=48, 1024+128]\n",
        "\n",
        "        if not h:\n",
        "            h = (torch.zeros((2, x.size(0), 512)).cuda(),\n",
        "                 torch.zeros((2, x.size(0), 512)).cuda())\n",
        "\n",
        "        x, _ = self.lstm(x, h)\n",
        "        x = self.l1(x)\n",
        "        # x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Pix2Code(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Pix2Code, self).__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.context_encoder = ContextEncoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, image, context):\n",
        "        image_feature = self.image_encoder(image)\n",
        "        context_feature = self.context_encoder(context)\n",
        "        output = self.decoder(image_feature, context_feature)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lcDXMzgkLgeb"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "PLACEHOLDER = ' '\n",
        "# CONTEXT_LENGTH = 48\n",
        "image_size = 256\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \n",
        "    def __init__(self, file_path):\n",
        "        self.load_vocab(file_path)\n",
        "        self.length = len(self.vocab_to_index)\n",
        "    \n",
        "    def load_vocab(self, file_path):\n",
        "        self.vocab_to_index = {}\n",
        "        with open(file_path, 'rb') as vocab_file:\n",
        "            self.vocab_to_index = pickle.load(vocab_file)\n",
        "        self.index_to_vocab = {value:key for key, value in self.vocab_to_index.items()}\n",
        "    \n",
        "    def to_vec(self, word):\n",
        "        vec = np.zeros(self.length)\n",
        "        vec[self.vocab_to_index[word]] = 1\n",
        "        return vec\n",
        "       \n",
        "    def to_vocab(self, index):\n",
        "        return self.index_to_vocab[index]\n",
        "\n",
        "class UIDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, file_path, vocab_file_path):\n",
        "        self.file_path = file_path\n",
        "        self.paths = []\n",
        "        self.get_paths()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize([image_size, image_size]),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        self.vocab = Vocabulary(vocab_file_path)\n",
        "        \n",
        "    def get_paths(self):\n",
        "        for f in os.listdir(self.file_path):\n",
        "            if f.find('.gui') != -1:\n",
        "                file_name = f[:f.find('.gui')]\n",
        "                if os.path.isfile('{}/{}.png'.format(self.file_path, file_name)):\n",
        "                    self.paths.append(file_name)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return(len(self.paths))\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.transform(Image.open('{}/{}.png'.format(self.file_path, self.paths[index])))[:-1]\n",
        "        context, prediction = self.read_gui('{}/{}.gui'.format(self.file_path, self.paths[index]))\n",
        "        return image, context, prediction\n",
        "    \n",
        "    def read_gui(self, file_path):\n",
        "        context = []\n",
        "        prediction = []\n",
        "        \n",
        "        # Tokenize the target code and ads start and end token\n",
        "        token_sequence = [PLACEHOLDER]\n",
        "        token_sequence.append(START_TOKEN)\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.replace(',', ' ,').replace('\\n', ' \\n')\n",
        "                tokens = line.split(' ')\n",
        "                for token in tokens:\n",
        "                    token_sequence.append(token)\n",
        "        token_sequence.append(END_TOKEN)\n",
        "        \n",
        "        # Generates cotext prediction pair\n",
        "        context = token_sequence[:-1]\n",
        "        prediction = token_sequence[1:]\n",
        "        \n",
        "        # suffix = [PLACEHOLDER] * CONTEXT_LENGTH\n",
        "        # a = np.concatenate([suffix, token_sequence])\n",
        "        # for j in range(len(token_sequence)):\n",
        "        #     # context.append(a[j:j + CONTEXT_LENGTH])\n",
        "        #     context.append(a[j])\n",
        "        #     prediction.append(a[j + CONTEXT_LENGTH])\n",
        "        \n",
        "        # One hot encoding\n",
        "        prediction_vec = []\n",
        "        for word in prediction:\n",
        "            prediction_vec.append(self.vocab.to_vec(word))\n",
        "        context_vec = []\n",
        "        for word in context:\n",
        "            context_vec.append(self.vocab.to_vec(word))\n",
        "        \n",
        "        return torch.tensor(context_vec, dtype=torch.float), torch.tensor(prediction_vec, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE3_mP2jFbdJ",
        "outputId": "8a24ec34-5e1c-431f-cf8c-99ca8aa12720"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-94845a433b1c>:91: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  return torch.tensor(context_vec, dtype=torch.float), torch.tensor(prediction_vec, dtype=torch.float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 5.026370525360107, Epoch: 0\n",
            "Loss: 5.01317024230957, Epoch: 0\n",
            "Loss: 5.001171588897705, Epoch: 0\n",
            "Loss: 4.980180263519287, Epoch: 0\n",
            "Loss: 4.958280086517334, Epoch: 0\n",
            "Loss: 4.917381286621094, Epoch: 0\n",
            "Loss: 4.864065170288086, Epoch: 0\n",
            "Loss: 4.792412757873535, Epoch: 0\n",
            "Loss: 4.68355131149292, Epoch: 0\n",
            "Loss: 4.528977870941162, Epoch: 0\n",
            "Loss: 4.33883810043335, Epoch: 0\n",
            "Loss: 4.083986759185791, Epoch: 0\n",
            "Loss: 3.8043971061706543, Epoch: 0\n",
            "Loss: 3.6360158920288086, Epoch: 0\n",
            "Loss: 3.488908052444458, Epoch: 0\n",
            "Loss: 3.195651054382324, Epoch: 0\n",
            "Loss: 3.228950262069702, Epoch: 0\n",
            "Loss: 3.054069757461548, Epoch: 0\n",
            "Loss: 3.0150747299194336, Epoch: 0\n",
            "Loss: 2.980863094329834, Epoch: 0\n",
            "Loss: 3.2150893211364746, Epoch: 0\n",
            "Loss: 2.923466444015503, Epoch: 0\n",
            "Loss: 3.0894882678985596, Epoch: 0\n",
            "Loss: 2.8549082279205322, Epoch: 0\n",
            "Loss: 2.7094056606292725, Epoch: 0\n",
            "Loss: 2.915930986404419, Epoch: 0\n",
            "Loss: 2.6768200397491455, Epoch: 0\n",
            "Loss: 2.8105759620666504, Epoch: 0\n",
            "Loss: 2.654010057449341, Epoch: 0\n",
            "Loss: 2.9291696548461914, Epoch: 0\n",
            "Loss: 2.704091787338257, Epoch: 0\n",
            "Loss: 2.6287970542907715, Epoch: 0\n",
            "Loss: 2.828601360321045, Epoch: 0\n",
            "Loss: 2.563248872756958, Epoch: 0\n",
            "Loss: 2.6363582611083984, Epoch: 0\n",
            "Loss: 2.643440008163452, Epoch: 0\n",
            "Loss: 2.716718912124634, Epoch: 0\n",
            "Loss: 2.546027421951294, Epoch: 0\n",
            "Loss: 2.5662479400634766, Epoch: 0\n",
            "Loss: 2.574536085128784, Epoch: 0\n",
            "Loss: 2.654743194580078, Epoch: 0\n",
            "Loss: 2.5476434230804443, Epoch: 0\n",
            "Loss: 2.5062882900238037, Epoch: 0\n",
            "Loss: 2.5361804962158203, Epoch: 0\n",
            "Loss: 2.6905324459075928, Epoch: 0\n",
            "Loss: 2.5336203575134277, Epoch: 0\n",
            "Loss: 2.469895362854004, Epoch: 0\n",
            "Loss: 2.5941734313964844, Epoch: 0\n",
            "Loss: 2.5349628925323486, Epoch: 0\n",
            "Loss: 2.598607301712036, Epoch: 0\n",
            "Loss: 2.476442813873291, Epoch: 0\n",
            "Loss: 2.5605547428131104, Epoch: 0\n",
            "Loss: 2.5125415325164795, Epoch: 0\n",
            "Loss: 2.6444106101989746, Epoch: 0\n",
            "Loss: 2.600179433822632, Epoch: 0\n",
            "Loss: 2.784071445465088, Epoch: 0\n",
            "Loss: 2.665154218673706, Epoch: 0\n",
            "Loss: 2.5379116535186768, Epoch: 0\n",
            "Loss: 2.4613864421844482, Epoch: 0\n",
            "Loss: 2.658764600753784, Epoch: 0\n",
            "Loss: 2.479020357131958, Epoch: 0\n",
            "Loss: 2.467205286026001, Epoch: 0\n",
            "Loss: 2.5323004722595215, Epoch: 0\n",
            "Loss: 2.5922117233276367, Epoch: 0\n",
            "Loss: 2.619503974914551, Epoch: 0\n",
            "Loss: 2.691636562347412, Epoch: 0\n",
            "Loss: 2.607724666595459, Epoch: 0\n",
            "Loss: 2.439661979675293, Epoch: 0\n",
            "Loss: 2.630309820175171, Epoch: 0\n",
            "Loss: 2.544217824935913, Epoch: 0\n",
            "Loss: 2.529346227645874, Epoch: 0\n",
            "Loss: 2.653691291809082, Epoch: 0\n",
            "Loss: 2.6196341514587402, Epoch: 0\n",
            "Loss: 2.636735677719116, Epoch: 0\n",
            "Loss: 2.681654214859009, Epoch: 0\n",
            "Loss: 2.6013638973236084, Epoch: 0\n",
            "Loss: 2.5331664085388184, Epoch: 0\n",
            "Loss: 2.4658520221710205, Epoch: 0\n",
            "Loss: 2.6064064502716064, Epoch: 0\n",
            "Loss: 2.608034133911133, Epoch: 0\n",
            "Loss: 2.608598470687866, Epoch: 0\n",
            "Loss: 2.537642002105713, Epoch: 0\n",
            "Loss: 2.4615697860717773, Epoch: 0\n",
            "Loss: 2.6449615955352783, Epoch: 0\n",
            "Loss: 2.5289289951324463, Epoch: 0\n",
            "Loss: 2.6613147258758545, Epoch: 0\n",
            "Loss: 2.68650484085083, Epoch: 0\n",
            "Loss: 2.544801950454712, Epoch: 0\n",
            "Loss: 2.49043607711792, Epoch: 0\n",
            "Loss: 2.6455490589141846, Epoch: 0\n",
            "Loss: 2.61364483833313, Epoch: 0\n",
            "Loss: 2.5059473514556885, Epoch: 0\n",
            "Loss: 2.5603747367858887, Epoch: 0\n",
            "Loss: 2.665879964828491, Epoch: 0\n",
            "Loss: 2.567021608352661, Epoch: 0\n",
            "Loss: 2.5655267238616943, Epoch: 0\n",
            "Loss: 2.5339345932006836, Epoch: 0\n",
            "Loss: 2.4527974128723145, Epoch: 0\n",
            "Loss: 2.9322469234466553, Epoch: 0\n",
            "Loss: 2.49829363822937, Epoch: 0\n",
            "Loss: 2.513824701309204, Epoch: 0\n",
            "Loss: 2.4634859561920166, Epoch: 0\n",
            "Loss: 2.428110361099243, Epoch: 0\n",
            "Loss: 2.4249348640441895, Epoch: 0\n",
            "Loss: 2.640596628189087, Epoch: 0\n",
            "Loss: 2.665188789367676, Epoch: 0\n",
            "Loss: 2.474963426589966, Epoch: 0\n",
            "Loss: 2.578864812850952, Epoch: 0\n",
            "Loss: 2.613081455230713, Epoch: 0\n",
            "Loss: 2.5114119052886963, Epoch: 0\n",
            "Loss: 2.478627920150757, Epoch: 0\n",
            "Loss: 2.510986089706421, Epoch: 0\n",
            "Loss: 2.539504051208496, Epoch: 0\n",
            "Loss: 2.4729464054107666, Epoch: 0\n",
            "Loss: 2.4668142795562744, Epoch: 0\n",
            "Loss: 2.457026720046997, Epoch: 0\n",
            "Loss: 2.6369824409484863, Epoch: 0\n",
            "Loss: 2.627002716064453, Epoch: 0\n",
            "Loss: 2.533015489578247, Epoch: 0\n",
            "Loss: 2.653250217437744, Epoch: 0\n",
            "Loss: 2.557743549346924, Epoch: 0\n",
            "Loss: 2.5638058185577393, Epoch: 0\n",
            "Loss: 2.5679023265838623, Epoch: 0\n",
            "Loss: 2.614919424057007, Epoch: 0\n",
            "Loss: 2.518338441848755, Epoch: 0\n",
            "Loss: 2.4298720359802246, Epoch: 0\n",
            "Loss: 2.4822494983673096, Epoch: 0\n",
            "Loss: 2.6947898864746094, Epoch: 0\n",
            "Loss: 2.481384038925171, Epoch: 0\n",
            "Loss: 2.595796823501587, Epoch: 0\n",
            "Loss: 2.5060625076293945, Epoch: 0\n",
            "Loss: 2.4208312034606934, Epoch: 0\n",
            "Loss: 2.450853109359741, Epoch: 0\n",
            "Loss: 2.5660572052001953, Epoch: 0\n",
            "Loss: 2.454564094543457, Epoch: 0\n",
            "Loss: 2.796703815460205, Epoch: 0\n",
            "Loss: 2.409196615219116, Epoch: 0\n",
            "Loss: 2.758516788482666, Epoch: 0\n",
            "Loss: 2.456943988800049, Epoch: 0\n",
            "Loss: 2.5685412883758545, Epoch: 0\n",
            "Loss: 2.46346116065979, Epoch: 0\n",
            "Loss: 2.6859593391418457, Epoch: 0\n",
            "Loss: 2.640328884124756, Epoch: 0\n",
            "Loss: 2.4602248668670654, Epoch: 0\n",
            "Loss: 2.5332133769989014, Epoch: 0\n",
            "Loss: 2.5072529315948486, Epoch: 0\n",
            "Loss: 2.4525363445281982, Epoch: 0\n",
            "Loss: 2.6470508575439453, Epoch: 0\n",
            "Loss: 2.550480604171753, Epoch: 0\n",
            "Loss: 2.5002386569976807, Epoch: 0\n",
            "Loss: 2.6578023433685303, Epoch: 0\n",
            "Loss: 2.5254929065704346, Epoch: 0\n",
            "Loss: 2.5542118549346924, Epoch: 0\n",
            "Loss: 2.6393792629241943, Epoch: 0\n",
            "Loss: 2.480703353881836, Epoch: 0\n",
            "Loss: 2.5859875679016113, Epoch: 0\n",
            "Loss: 2.5886478424072266, Epoch: 0\n",
            "Loss: 2.487787961959839, Epoch: 0\n",
            "Loss: 2.5791244506835938, Epoch: 0\n",
            "Loss: 2.603463888168335, Epoch: 0\n",
            "Loss: 2.4318349361419678, Epoch: 0\n",
            "Loss: 2.8374335765838623, Epoch: 0\n",
            "Loss: 2.4821951389312744, Epoch: 0\n",
            "Loss: 2.517984390258789, Epoch: 0\n",
            "Loss: 2.5165185928344727, Epoch: 0\n",
            "Loss: 2.559842586517334, Epoch: 0\n",
            "Loss: 2.5570430755615234, Epoch: 0\n",
            "Loss: 2.5552477836608887, Epoch: 0\n",
            "Loss: 2.7266359329223633, Epoch: 0\n",
            "Loss: 2.4443747997283936, Epoch: 0\n",
            "Loss: 2.540830612182617, Epoch: 0\n",
            "Loss: 2.4958136081695557, Epoch: 0\n",
            "Loss: 2.3921940326690674, Epoch: 0\n",
            "Loss: 2.5255038738250732, Epoch: 0\n",
            "Loss: 2.4515724182128906, Epoch: 0\n",
            "Loss: 2.4491004943847656, Epoch: 0\n",
            "Loss: 2.5838429927825928, Epoch: 0\n",
            "Loss: 2.4510960578918457, Epoch: 0\n",
            "Loss: 2.527221441268921, Epoch: 0\n",
            "Loss: 2.4279940128326416, Epoch: 0\n",
            "Loss: 2.425762891769409, Epoch: 0\n",
            "Loss: 2.5698912143707275, Epoch: 0\n",
            "Loss: 2.516087770462036, Epoch: 0\n",
            "Loss: 2.688171625137329, Epoch: 0\n",
            "Loss: 2.424117088317871, Epoch: 0\n",
            "Loss: 2.5519516468048096, Epoch: 0\n",
            "Loss: 2.473273515701294, Epoch: 0\n",
            "Loss: 2.445672035217285, Epoch: 0\n",
            "Loss: 2.5687737464904785, Epoch: 0\n",
            "Loss: 2.453528881072998, Epoch: 0\n",
            "Loss: 2.4300451278686523, Epoch: 0\n",
            "Loss: 2.4650051593780518, Epoch: 0\n",
            "Loss: 2.602104425430298, Epoch: 0\n",
            "Loss: 2.60701847076416, Epoch: 0\n",
            "Loss: 2.4522135257720947, Epoch: 0\n",
            "Loss: 2.4929702281951904, Epoch: 0\n",
            "Loss: 2.417489528656006, Epoch: 0\n",
            "Loss: 2.6409881114959717, Epoch: 0\n",
            "Loss: 2.4984359741210938, Epoch: 0\n",
            "Loss: 2.4707069396972656, Epoch: 0\n",
            "Loss: 2.442781448364258, Epoch: 0\n",
            "Loss: 2.4516358375549316, Epoch: 0\n",
            "Loss: 2.509904146194458, Epoch: 0\n",
            "Loss: 2.6010451316833496, Epoch: 0\n",
            "Loss: 2.4924302101135254, Epoch: 0\n",
            "Loss: 2.4834394454956055, Epoch: 0\n",
            "Loss: 2.4399185180664062, Epoch: 0\n",
            "Loss: 2.4326255321502686, Epoch: 0\n",
            "Loss: 2.4635908603668213, Epoch: 0\n",
            "Loss: 2.5180749893188477, Epoch: 0\n",
            "Loss: 2.498178482055664, Epoch: 0\n",
            "Loss: 2.446380376815796, Epoch: 0\n",
            "Loss: 2.4760587215423584, Epoch: 0\n",
            "Loss: 2.4648830890655518, Epoch: 0\n",
            "Loss: 2.422572612762451, Epoch: 0\n",
            "Loss: 2.416602611541748, Epoch: 0\n",
            "Loss: 2.4161627292633057, Epoch: 0\n",
            "Loss: 2.415045738220215, Epoch: 0\n",
            "Loss: 2.4572649002075195, Epoch: 0\n",
            "Loss: 2.431852102279663, Epoch: 0\n",
            "Loss: 2.4989173412323, Epoch: 0\n",
            "Loss: 2.589083671569824, Epoch: 0\n",
            "Loss: 2.418220281600952, Epoch: 0\n",
            "Loss: 2.4398138523101807, Epoch: 0\n",
            "Loss: 2.406257152557373, Epoch: 0\n",
            "Loss: 2.408536195755005, Epoch: 0\n",
            "Loss: 2.4988045692443848, Epoch: 0\n",
            "Loss: 2.470625400543213, Epoch: 0\n",
            "Loss: 2.4016997814178467, Epoch: 0\n",
            "Loss: 2.521976947784424, Epoch: 0\n",
            "Loss: 2.450652837753296, Epoch: 0\n",
            "Loss: 2.4870874881744385, Epoch: 0\n",
            "Loss: 2.494691848754883, Epoch: 0\n",
            "Loss: 2.4382948875427246, Epoch: 0\n",
            "Loss: 2.4365131855010986, Epoch: 0\n",
            "Loss: 2.415017604827881, Epoch: 0\n",
            "Loss: 2.4987130165100098, Epoch: 0\n",
            "Loss: 2.424901247024536, Epoch: 0\n",
            "Loss: 2.4516191482543945, Epoch: 0\n",
            "Loss: 2.4711813926696777, Epoch: 0\n",
            "Loss: 2.4783332347869873, Epoch: 0\n",
            "Loss: 2.450141668319702, Epoch: 0\n",
            "Loss: 2.4375429153442383, Epoch: 0\n",
            "Loss: 2.462341070175171, Epoch: 0\n",
            "Loss: 2.443284034729004, Epoch: 0\n",
            "Loss: 2.503872871398926, Epoch: 0\n",
            "Loss: 2.5092053413391113, Epoch: 0\n",
            "Loss: 2.63915753364563, Epoch: 0\n",
            "Loss: 2.487603187561035, Epoch: 0\n",
            "Loss: 2.52764630317688, Epoch: 0\n",
            "Loss: 2.3818724155426025, Epoch: 0\n",
            "Loss: 2.617887020111084, Epoch: 0\n",
            "Loss: 2.5474750995635986, Epoch: 0\n",
            "Loss: 2.4917285442352295, Epoch: 0\n",
            "Loss: 2.4015448093414307, Epoch: 0\n",
            "Loss: 2.425124406814575, Epoch: 0\n",
            "Loss: 2.5043957233428955, Epoch: 0\n",
            "Loss: 2.3760485649108887, Epoch: 0\n",
            "Loss: 2.398599147796631, Epoch: 0\n",
            "Loss: 2.577165126800537, Epoch: 0\n",
            "Loss: 2.4527323246002197, Epoch: 0\n",
            "Loss: 2.444279193878174, Epoch: 0\n",
            "Loss: 2.414612293243408, Epoch: 0\n",
            "Loss: 2.4089691638946533, Epoch: 0\n",
            "Loss: 2.4082913398742676, Epoch: 0\n",
            "Loss: 2.4109015464782715, Epoch: 0\n",
            "Loss: 2.4427199363708496, Epoch: 0\n",
            "Loss: 2.5859501361846924, Epoch: 0\n",
            "Loss: 2.463381052017212, Epoch: 0\n",
            "Loss: 2.5531516075134277, Epoch: 0\n",
            "Loss: 2.393498182296753, Epoch: 0\n",
            "Loss: 2.390207290649414, Epoch: 0\n",
            "Loss: 2.5207626819610596, Epoch: 0\n",
            "Loss: 2.3590214252471924, Epoch: 0\n",
            "Loss: 2.5122108459472656, Epoch: 0\n",
            "Loss: 2.4415853023529053, Epoch: 0\n",
            "Loss: 2.4367904663085938, Epoch: 0\n",
            "Loss: 2.4197945594787598, Epoch: 0\n",
            "Loss: 2.4256081581115723, Epoch: 0\n",
            "Loss: 2.4409353733062744, Epoch: 0\n",
            "Loss: 2.3964638710021973, Epoch: 0\n",
            "Loss: 2.485410213470459, Epoch: 0\n",
            "Loss: 2.4526829719543457, Epoch: 0\n",
            "Loss: 2.5295419692993164, Epoch: 0\n",
            "Loss: 2.419076919555664, Epoch: 0\n",
            "Loss: 2.5359718799591064, Epoch: 0\n",
            "Loss: 2.35560941696167, Epoch: 0\n",
            "Loss: 2.409674644470215, Epoch: 0\n",
            "Loss: 2.5856101512908936, Epoch: 0\n",
            "Loss: 2.389402151107788, Epoch: 0\n",
            "Loss: 2.4304676055908203, Epoch: 0\n",
            "Loss: 2.4301156997680664, Epoch: 0\n",
            "Loss: 2.3995609283447266, Epoch: 0\n",
            "Loss: 2.4773671627044678, Epoch: 0\n",
            "Loss: 2.5526134967803955, Epoch: 0\n",
            "Loss: 2.4248745441436768, Epoch: 0\n",
            "Loss: 2.3948562145233154, Epoch: 0\n",
            "Loss: 2.4423768520355225, Epoch: 0\n",
            "Loss: 2.4541304111480713, Epoch: 0\n",
            "Loss: 2.3900411128997803, Epoch: 0\n",
            "Loss: 2.4116086959838867, Epoch: 0\n",
            "Loss: 2.490856647491455, Epoch: 0\n",
            "Loss: 2.392235517501831, Epoch: 0\n",
            "Loss: 2.372183084487915, Epoch: 0\n",
            "Loss: 2.4233970642089844, Epoch: 0\n",
            "Loss: 2.478902578353882, Epoch: 0\n",
            "Loss: 2.3900644779205322, Epoch: 0\n",
            "Loss: 2.464785099029541, Epoch: 0\n",
            "Loss: 2.4708263874053955, Epoch: 0\n",
            "Loss: 2.4130749702453613, Epoch: 0\n",
            "Loss: 2.396794557571411, Epoch: 0\n",
            "Loss: 2.4285452365875244, Epoch: 0\n",
            "Loss: 2.4412176609039307, Epoch: 0\n",
            "Loss: 2.4313530921936035, Epoch: 0\n",
            "Loss: 2.4073829650878906, Epoch: 0\n",
            "Loss: 2.488870620727539, Epoch: 0\n",
            "Loss: 2.478793144226074, Epoch: 0\n",
            "Loss: 2.4297842979431152, Epoch: 0\n",
            "Loss: 2.4178507328033447, Epoch: 0\n",
            "Loss: 2.396388053894043, Epoch: 0\n",
            "Loss: 2.465365409851074, Epoch: 0\n",
            "Loss: 2.444427251815796, Epoch: 0\n",
            "Loss: 2.391294002532959, Epoch: 0\n",
            "Loss: 2.4292356967926025, Epoch: 0\n",
            "Loss: 2.365978956222534, Epoch: 0\n",
            "Loss: 2.4118785858154297, Epoch: 0\n",
            "Loss: 2.379589557647705, Epoch: 0\n",
            "Loss: 2.4417383670806885, Epoch: 0\n",
            "Loss: 2.441688060760498, Epoch: 0\n",
            "Loss: 2.3752641677856445, Epoch: 0\n",
            "Loss: 2.367447853088379, Epoch: 0\n",
            "Loss: 2.419959545135498, Epoch: 0\n",
            "Loss: 2.499265193939209, Epoch: 0\n",
            "Loss: 2.340492010116577, Epoch: 0\n",
            "Loss: 2.3666906356811523, Epoch: 0\n",
            "Loss: 2.3673930168151855, Epoch: 0\n",
            "Loss: 2.4044995307922363, Epoch: 0\n",
            "Loss: 2.4722070693969727, Epoch: 0\n",
            "Loss: 2.4280385971069336, Epoch: 0\n",
            "Loss: 2.3889338970184326, Epoch: 0\n",
            "Loss: 2.4280295372009277, Epoch: 0\n",
            "Loss: 2.405611515045166, Epoch: 0\n",
            "Loss: 2.377258062362671, Epoch: 0\n",
            "Loss: 2.4140117168426514, Epoch: 0\n",
            "Loss: 2.509920835494995, Epoch: 0\n",
            "Loss: 2.377241373062134, Epoch: 0\n",
            "Loss: 2.4609336853027344, Epoch: 0\n",
            "Loss: 2.4359514713287354, Epoch: 0\n",
            "Loss: 2.46638822555542, Epoch: 0\n",
            "Loss: 2.4049293994903564, Epoch: 0\n",
            "Loss: 2.3519845008850098, Epoch: 0\n",
            "Loss: 2.42350435256958, Epoch: 0\n",
            "Loss: 2.3925509452819824, Epoch: 0\n",
            "Loss: 2.4345483779907227, Epoch: 0\n",
            "Loss: 2.3753304481506348, Epoch: 0\n",
            "Loss: 2.401991128921509, Epoch: 0\n",
            "Loss: 2.3705031871795654, Epoch: 0\n",
            "Loss: 2.3546555042266846, Epoch: 0\n",
            "Loss: 2.4846301078796387, Epoch: 0\n",
            "Loss: 2.4054226875305176, Epoch: 0\n",
            "Loss: 2.325471878051758, Epoch: 0\n",
            "Loss: 2.476825714111328, Epoch: 0\n",
            "Loss: 2.522677421569824, Epoch: 0\n",
            "Loss: 2.414299726486206, Epoch: 0\n",
            "Loss: 2.3318183422088623, Epoch: 0\n",
            "Loss: 2.430450677871704, Epoch: 0\n",
            "Loss: 2.397263526916504, Epoch: 0\n",
            "Loss: 2.383798837661743, Epoch: 0\n",
            "Loss: 2.4142308235168457, Epoch: 0\n",
            "Loss: 2.420088768005371, Epoch: 0\n",
            "Loss: 2.3852999210357666, Epoch: 0\n",
            "Loss: 2.4123990535736084, Epoch: 0\n",
            "Loss: 2.3589296340942383, Epoch: 0\n",
            "Loss: 2.3723654747009277, Epoch: 0\n",
            "Loss: 2.3348581790924072, Epoch: 0\n",
            "Loss: 2.3666930198669434, Epoch: 0\n",
            "Loss: 2.353750705718994, Epoch: 0\n",
            "Loss: 2.3908722400665283, Epoch: 0\n",
            "Loss: 2.357844352722168, Epoch: 0\n",
            "Loss: 2.324331760406494, Epoch: 0\n",
            "Loss: 2.3761520385742188, Epoch: 0\n",
            "Loss: 2.4082653522491455, Epoch: 0\n",
            "Loss: 2.403160572052002, Epoch: 0\n",
            "Loss: 2.391662359237671, Epoch: 0\n",
            "Loss: 2.410813093185425, Epoch: 0\n",
            "Loss: 2.3527092933654785, Epoch: 0\n",
            "Loss: 2.377227544784546, Epoch: 0\n",
            "Loss: 2.357814073562622, Epoch: 0\n",
            "Loss: 2.3376097679138184, Epoch: 0\n",
            "Loss: 2.4728565216064453, Epoch: 0\n",
            "Loss: 2.3291633129119873, Epoch: 0\n",
            "Loss: 2.364182949066162, Epoch: 0\n",
            "Loss: 2.363509178161621, Epoch: 0\n",
            "Loss: 2.3905653953552246, Epoch: 0\n",
            "Loss: 2.3491201400756836, Epoch: 0\n",
            "Loss: 2.3757383823394775, Epoch: 0\n",
            "Loss: 2.3210527896881104, Epoch: 0\n",
            "Loss: 2.4014503955841064, Epoch: 0\n",
            "Loss: 2.36488676071167, Epoch: 0\n",
            "Loss: 2.387113571166992, Epoch: 0\n",
            "Loss: 2.345681667327881, Epoch: 0\n",
            "Loss: 2.3897407054901123, Epoch: 0\n",
            "Loss: 2.349003314971924, Epoch: 0\n",
            "Loss: 2.3358819484710693, Epoch: 0\n",
            "Loss: 2.3403971195220947, Epoch: 0\n",
            "Loss: 2.44281005859375, Epoch: 0\n",
            "Loss: 2.3494584560394287, Epoch: 0\n",
            "Loss: 2.3401105403900146, Epoch: 0\n",
            "Loss: 2.3700451850891113, Epoch: 0\n",
            "Loss: 2.4277656078338623, Epoch: 0\n",
            "Loss: 2.3724167346954346, Epoch: 0\n",
            "Loss: 2.351625919342041, Epoch: 0\n",
            "Loss: 2.301661252975464, Epoch: 0\n",
            "Loss: 2.3721911907196045, Epoch: 0\n",
            "Loss: 2.3388898372650146, Epoch: 0\n",
            "Loss: 2.351637601852417, Epoch: 0\n",
            "Loss: 2.379354953765869, Epoch: 0\n",
            "Loss: 2.389008045196533, Epoch: 0\n",
            "Loss: 2.3802764415740967, Epoch: 0\n",
            "Loss: 2.3915648460388184, Epoch: 0\n",
            "Loss: 2.3889353275299072, Epoch: 0\n",
            "Loss: 2.372070550918579, Epoch: 0\n",
            "Loss: 2.370248317718506, Epoch: 0\n",
            "Loss: 2.318014621734619, Epoch: 0\n",
            "Loss: 2.3733108043670654, Epoch: 0\n",
            "Loss: 2.3455262184143066, Epoch: 0\n",
            "Loss: 2.323373317718506, Epoch: 0\n",
            "Loss: 2.420541763305664, Epoch: 0\n",
            "Loss: 2.3254499435424805, Epoch: 0\n",
            "Loss: 2.353438377380371, Epoch: 0\n",
            "Loss: 2.366939067840576, Epoch: 0\n",
            "Loss: 2.383615732192993, Epoch: 0\n",
            "Loss: 2.3638219833374023, Epoch: 0\n",
            "Loss: 2.3299479484558105, Epoch: 0\n",
            "Loss: 2.3588085174560547, Epoch: 0\n",
            "Loss: 2.394791841506958, Epoch: 0\n",
            "Loss: 2.3209574222564697, Epoch: 0\n",
            "Loss: 2.3411452770233154, Epoch: 0\n",
            "Loss: 2.4318954944610596, Epoch: 0\n",
            "Loss: 2.3664398193359375, Epoch: 0\n",
            "Loss: 2.357060432434082, Epoch: 0\n",
            "Loss: 2.3845295906066895, Epoch: 0\n",
            "Loss: 2.3509716987609863, Epoch: 0\n",
            "Loss: 2.3361802101135254, Epoch: 0\n",
            "Loss: 2.4620277881622314, Epoch: 0\n",
            "Loss: 2.2984659671783447, Epoch: 0\n",
            "Loss: 2.366122007369995, Epoch: 0\n",
            "Loss: 2.3970160484313965, Epoch: 0\n",
            "Loss: 2.314833641052246, Epoch: 0\n",
            "Loss: 2.3406691551208496, Epoch: 0\n",
            "Loss: 2.3826863765716553, Epoch: 0\n",
            "Loss: 2.34140944480896, Epoch: 0\n",
            "Loss: 2.3294193744659424, Epoch: 0\n",
            "Loss: 2.3928446769714355, Epoch: 0\n",
            "Loss: 2.3326570987701416, Epoch: 0\n",
            "Loss: 2.365962266921997, Epoch: 0\n",
            "Loss: 2.3096673488616943, Epoch: 0\n",
            "Loss: 2.358877658843994, Epoch: 0\n",
            "Loss: 2.2989730834960938, Epoch: 0\n",
            "Loss: 2.3771727085113525, Epoch: 0\n",
            "Loss: 2.369194269180298, Epoch: 0\n",
            "Loss: 2.34914231300354, Epoch: 0\n",
            "Loss: 2.2990715503692627, Epoch: 0\n",
            "Loss: 2.276411533355713, Epoch: 0\n",
            "Loss: 2.390984058380127, Epoch: 0\n",
            "Loss: 2.35787296295166, Epoch: 0\n",
            "Loss: 2.2800610065460205, Epoch: 0\n",
            "Loss: 2.2938008308410645, Epoch: 0\n",
            "Loss: 2.279062032699585, Epoch: 0\n",
            "Loss: 2.4414992332458496, Epoch: 0\n",
            "Loss: 2.3705363273620605, Epoch: 0\n",
            "Loss: 2.402418375015259, Epoch: 0\n",
            "Loss: 2.3604202270507812, Epoch: 0\n",
            "Loss: 2.2942163944244385, Epoch: 0\n",
            "Loss: 2.274608612060547, Epoch: 0\n",
            "Loss: 2.391488552093506, Epoch: 0\n",
            "Loss: 2.361654043197632, Epoch: 0\n",
            "Loss: 2.395641565322876, Epoch: 0\n",
            "Loss: 2.344231367111206, Epoch: 0\n",
            "Loss: 2.3603484630584717, Epoch: 0\n",
            "Loss: 2.3088393211364746, Epoch: 0\n",
            "Loss: 2.30769681930542, Epoch: 0\n",
            "Loss: 2.420260429382324, Epoch: 0\n",
            "Loss: 2.3205106258392334, Epoch: 0\n",
            "Loss: 2.285449981689453, Epoch: 0\n",
            "Loss: 2.4472787380218506, Epoch: 0\n",
            "Loss: 2.452399969100952, Epoch: 0\n",
            "Loss: 2.330767869949341, Epoch: 0\n",
            "Loss: 2.2992143630981445, Epoch: 0\n",
            "Loss: 2.3056154251098633, Epoch: 0\n",
            "Loss: 2.3418610095977783, Epoch: 0\n",
            "Loss: 2.3507936000823975, Epoch: 0\n",
            "Loss: 2.386046886444092, Epoch: 0\n",
            "Loss: 2.392514705657959, Epoch: 0\n",
            "Loss: 2.324333667755127, Epoch: 0\n",
            "Loss: 2.3295063972473145, Epoch: 0\n",
            "Loss: 2.30547833442688, Epoch: 0\n",
            "Loss: 2.3157224655151367, Epoch: 0\n",
            "Loss: 2.3193325996398926, Epoch: 0\n",
            "Loss: 2.568646192550659, Epoch: 0\n",
            "Loss: 2.3369498252868652, Epoch: 0\n",
            "Loss: 2.40411114692688, Epoch: 0\n",
            "Loss: 2.354257583618164, Epoch: 0\n",
            "Loss: 2.3886823654174805, Epoch: 0\n",
            "Loss: 2.383920431137085, Epoch: 0\n",
            "Loss: 2.2845258712768555, Epoch: 0\n",
            "Loss: 2.3601481914520264, Epoch: 0\n",
            "Loss: 2.443769931793213, Epoch: 0\n",
            "Loss: 2.2958528995513916, Epoch: 0\n",
            "Loss: 2.3463852405548096, Epoch: 0\n",
            "Loss: 2.3395841121673584, Epoch: 0\n",
            "Loss: 2.3027541637420654, Epoch: 0\n",
            "Loss: 2.3334453105926514, Epoch: 0\n",
            "Loss: 2.386080026626587, Epoch: 0\n",
            "Loss: 2.339647054672241, Epoch: 0\n",
            "Loss: 2.2804477214813232, Epoch: 0\n",
            "Loss: 2.3060271739959717, Epoch: 0\n",
            "Loss: 2.346186399459839, Epoch: 0\n",
            "Loss: 2.3166773319244385, Epoch: 0\n",
            "Loss: 2.3623645305633545, Epoch: 0\n",
            "Loss: 2.3266284465789795, Epoch: 0\n",
            "Loss: 2.301530122756958, Epoch: 0\n",
            "Loss: 2.3392114639282227, Epoch: 0\n",
            "Loss: 2.3401405811309814, Epoch: 0\n"
          ]
        }
      ],
      "source": [
        "dataset = UIDataset('/content/drive/MyDrive/Senior/V6', '/content/voc.pkl')\n",
        "\n",
        "# Training\n",
        "net = Pix2Code().cuda()\n",
        "# net.load_state_dict(torch.load('/content/pix2code_v2.weights'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr = 0.0001)\n",
        "\n",
        "\n",
        "for epoch in range(2):\n",
        "    net.zero_grad()\n",
        "    for j, data in enumerate(dataset):\n",
        "        image, context, prediction = data\n",
        "        image = image.unsqueeze(0).cuda()\n",
        "        context = context.unsqueeze(0).cuda()\n",
        "        prediction = prediction.cuda()\n",
        "        output = net(image, context)\n",
        "        output = output.squeeze(0)\n",
        "        prediction = torch.argmax(prediction, 1)\n",
        "        loss = criterion(output, prediction)\n",
        "        loss.backward()\n",
        "        if j%10 == 0:\n",
        "            optimizer.step()\n",
        "            print('Loss: {}, Epoch: {}'.format(loss.data, epoch))\n",
        "            net.zero_grad()\n",
        "\n",
        "torch.save(net.state_dict(), './pix2code_LSTM.weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjZHzw1LeMxE"
      },
      "outputs": [],
      "source": [
        "!cp /content/pix2code_LSTM.weights /content/drive/MyDrive/Senior/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puDPMYI5inP8"
      },
      "outputs": [],
      "source": [
        "net = Pix2Code()\n",
        "net.load_state_dict(torch.load('/content/pix2code_LSTM.weights'))\n",
        "net.cuda().eval()\n",
        "test_data = UIDataset('/content/test', '/content/voc.pkl')\n",
        "vocab = Vocabulary('/content/voc.pkl')\n",
        "\n",
        "image, *_ = test_data.__getitem__(np.random.randint(len(test_data)))\n",
        "\n",
        "t = transforms.ToPILImage()\n",
        "image = image.unsqueeze(0)\n",
        "t(image.squeeze())\n",
        "\n",
        "image = image.cuda()\n",
        "ct = []\n",
        "ct.append(vocab.to_vec(' '))\n",
        "ct.append(vocab.to_vec('<START>'))\n",
        "output = ''\n",
        "for i in range(200):\n",
        "    context = torch.tensor(ct).unsqueeze(0).float().cuda()\n",
        "    index = torch.argmax(net(image, context), 2).squeeze()[-1:].squeeze()\n",
        "    v = vocab.to_vocab(int(index))\n",
        "    if v == '<END>':\n",
        "        break\n",
        "    output += v\n",
        "    ct.append(vocab.to_vec(v))\n",
        "\n",
        "with open('/content/output.gui', 'w') as f:\n",
        "    f.write(output)\n",
        "\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
